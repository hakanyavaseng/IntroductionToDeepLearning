{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtzZziEaOq3O",
        "outputId": "79449312-0be2-49f9-829f-06c038b0df82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if 1:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaQ7plS8O4uy",
        "outputId": "cb285a1f-daa2-4e9e-f943-3a826c9ca986"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Loading data from Google Drive\n",
        "X_train = np.load('/content/drive/MyDrive/Deep_Learning/data/DataForClassification_TimeDomain.npy')\n",
        "X_train = np.transpose(X_train)\n",
        "\n",
        "# Generating labels\n",
        "labels = np.zeros((936, 1))\n",
        "label = 0\n",
        "for i in range(936):\n",
        "    labels[i] = label\n",
        "    if (i % 104 == 0) and (i != 0):\n",
        "        label = label + 1\n",
        "\n",
        "# Encoding labels using OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "Y_train = encoder.fit_transform(labels)\n",
        "\n",
        "# Train-Test Split\n",
        "X_dev, X_test, Y_dev, Y_test = train_test_split(X_train, Y_train, test_size=0.2, shuffle=True, random_state=0)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_dev, Y_dev, test_size=0.2, shuffle=True, random_state=0)\n",
        "\n",
        "# Converting data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
        "Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
        "\n",
        "# Expanding dimensions\n",
        "X_train = X_train.unsqueeze(1)\n",
        "X_val = X_val.unsqueeze(1)\n",
        "X_test = X_test.unsqueeze(1)\n",
        "\n",
        "# Defining model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, 8)\n",
        "        self.fc2 = nn.Linear(8, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc1(out[:, -1, :])\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Initializing model\n",
        "model = GRUModel(input_size=3600, hidden_size=32, num_classes=9)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) #Used Adam like in Keras code\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.3, patience=7, min_lr=1e-5, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = TensorDataset(X_train, torch.argmax(Y_train, dim=1))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_val)\n",
        "        val_loss = criterion(val_outputs, torch.argmax(Y_val, dim=1))\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "# Saving the model\n",
        "torch.save(model.state_dict(), '/content/nn_model.pth')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(X_test)\n",
        "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
        "\n",
        "# Converting predictions to NumPy array\n",
        "test_predictions_np = test_predictions.numpy()\n",
        "print(test_predictions_np)\n",
        "\n",
        "\n",
        "# Doing this to see the accuracy on the test set\n",
        "# Converting predictions to NumPy array\n",
        "test_predictions_np = test_predictions.numpy()\n",
        "\n",
        "# Converting ground truth labels to NumPy array\n",
        "Y_test_np = torch.argmax(Y_test, dim=1).numpy()\n",
        "\n",
        "# Comparing predictions with ground truth\n",
        "correct_predictions = (test_predictions_np == Y_test_np)\n",
        "accuracy = correct_predictions.sum() / len(correct_predictions)\n",
        "\n",
        "\n",
        "print(f\"\\n\\nAccuracy on the test set: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiwNNa0iPLIh",
        "outputId": "767d5a71-3cd2-413b-8117-a606ac4f4ba0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.086857318878174, Validation Loss: 2.0705394744873047\n",
            "Epoch 2/50, Loss: 1.8625259399414062, Validation Loss: 1.9216210842132568\n",
            "Epoch 3/50, Loss: 1.7233718633651733, Validation Loss: 1.7642244100570679\n",
            "Epoch 4/50, Loss: 1.6051139831542969, Validation Loss: 1.5986182689666748\n",
            "Epoch 5/50, Loss: 1.1611168384552002, Validation Loss: 1.432564377784729\n",
            "Epoch 6/50, Loss: 1.2560884952545166, Validation Loss: 1.2694659233093262\n",
            "Epoch 7/50, Loss: 1.016408920288086, Validation Loss: 1.117722988128662\n",
            "Epoch 8/50, Loss: 0.7916843295097351, Validation Loss: 0.9815737009048462\n",
            "Epoch 00009: reducing learning rate of group 0 to 3.0000e-04.\n",
            "Epoch 9/50, Loss: 0.5585638284683228, Validation Loss: 0.8596180081367493\n",
            "Epoch 10/50, Loss: 0.7521899938583374, Validation Loss: 0.8278857469558716\n",
            "Epoch 11/50, Loss: 0.7283679842948914, Validation Loss: 0.796939492225647\n",
            "Epoch 12/50, Loss: 0.6772805452346802, Validation Loss: 0.7683131694793701\n",
            "Epoch 13/50, Loss: 0.44520238041877747, Validation Loss: 0.7410909533500671\n",
            "Epoch 14/50, Loss: 0.4900144338607788, Validation Loss: 0.7151849865913391\n",
            "Epoch 15/50, Loss: 0.5588996410369873, Validation Loss: 0.6898095011711121\n",
            "Epoch 16/50, Loss: 0.40859314799308777, Validation Loss: 0.664591908454895\n",
            "Epoch 00017: reducing learning rate of group 0 to 9.0000e-05.\n",
            "Epoch 17/50, Loss: 0.46066930890083313, Validation Loss: 0.6408618688583374\n",
            "Epoch 18/50, Loss: 0.45271456241607666, Validation Loss: 0.6339852809906006\n",
            "Epoch 19/50, Loss: 0.5441949367523193, Validation Loss: 0.6269959211349487\n",
            "Epoch 20/50, Loss: 0.5697023868560791, Validation Loss: 0.6201631426811218\n",
            "Epoch 21/50, Loss: 0.4515496492385864, Validation Loss: 0.6136181354522705\n",
            "Epoch 22/50, Loss: 0.45820674300193787, Validation Loss: 0.6070429682731628\n",
            "Epoch 23/50, Loss: 0.36589518189430237, Validation Loss: 0.6005030274391174\n",
            "Epoch 24/50, Loss: 0.4097661077976227, Validation Loss: 0.5940194725990295\n",
            "Epoch 00025: reducing learning rate of group 0 to 2.7000e-05.\n",
            "Epoch 25/50, Loss: 0.4703369438648224, Validation Loss: 0.5873796939849854\n",
            "Epoch 26/50, Loss: 0.36723801493644714, Validation Loss: 0.5854431986808777\n",
            "Epoch 27/50, Loss: 0.35012051463127136, Validation Loss: 0.5835618376731873\n",
            "Epoch 28/50, Loss: 0.3033045828342438, Validation Loss: 0.5815871357917786\n",
            "Epoch 29/50, Loss: 0.34826695919036865, Validation Loss: 0.5797553062438965\n",
            "Epoch 30/50, Loss: 0.39166784286499023, Validation Loss: 0.5778140425682068\n",
            "Epoch 31/50, Loss: 0.28131362795829773, Validation Loss: 0.5758578777313232\n",
            "Epoch 32/50, Loss: 0.4652253985404968, Validation Loss: 0.5739786028862\n",
            "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Epoch 33/50, Loss: 0.432069331407547, Validation Loss: 0.5720459222793579\n",
            "Epoch 34/50, Loss: 0.5467842817306519, Validation Loss: 0.5713237524032593\n",
            "Epoch 35/50, Loss: 0.3363731801509857, Validation Loss: 0.57063227891922\n",
            "Epoch 36/50, Loss: 0.3910658657550812, Validation Loss: 0.5699326395988464\n",
            "Epoch 37/50, Loss: 0.38766202330589294, Validation Loss: 0.5692054033279419\n",
            "Epoch 38/50, Loss: 0.39870694279670715, Validation Loss: 0.5684806704521179\n",
            "Epoch 39/50, Loss: 0.4152584373950958, Validation Loss: 0.5677123069763184\n",
            "Epoch 40/50, Loss: 0.4360458254814148, Validation Loss: 0.5669957995414734\n",
            "Epoch 41/50, Loss: 0.3512445092201233, Validation Loss: 0.5662712454795837\n",
            "Epoch 42/50, Loss: 0.49172353744506836, Validation Loss: 0.5655346512794495\n",
            "Epoch 43/50, Loss: 0.418962687253952, Validation Loss: 0.5647993683815002\n",
            "Epoch 44/50, Loss: 0.342530757188797, Validation Loss: 0.5640906691551208\n",
            "Epoch 45/50, Loss: 0.31310322880744934, Validation Loss: 0.5633641481399536\n",
            "Epoch 46/50, Loss: 0.34315383434295654, Validation Loss: 0.5626309514045715\n",
            "Epoch 47/50, Loss: 0.3711567223072052, Validation Loss: 0.5619452595710754\n",
            "Epoch 48/50, Loss: 0.4273695945739746, Validation Loss: 0.5611960887908936\n",
            "Epoch 49/50, Loss: 0.32957547903060913, Validation Loss: 0.5604882836341858\n",
            "Epoch 50/50, Loss: 0.33650845289230347, Validation Loss: 0.5597543716430664\n",
            "[7 2 7 7 2 0 5 7 0 3 7 8 2 0 3 6 7 6 1 6 6 7 3 4 1 5 1 2 2 6 1 6 4 5 2 4 0\n",
            " 8 4 4 2 3 3 0 2 1 7 4 0 3 7 7 7 6 5 8 3 2 3 4 4 4 3 1 0 2 5 4 4 1 7 0 4 2\n",
            " 3 4 0 5 4 1 5 3 6 5 0 7 6 8 2 8 3 6 4 3 5 0 6 8 6 2 6 5 0 6 0 7 4 7 6 7 5\n",
            " 8 2 6 5 1 3 2 2 1 4 7 8 7 5 5 3 8 6 2 6 5 3 7 3 7 4 6 2 2 4 0 6 4 8 3 2 6\n",
            " 3 1 4 6 4 8 7 1 5 0 8 6 1 8 0 6 2 7 5 8 5 4 2 7 4 1 6 8 0 0 7 3 4 2 8 5 6\n",
            " 0 4 3]\n",
            "\n",
            "\n",
            "Accuracy on the test set: 95.74%\n"
          ]
        }
      ]
    }
  ]
}